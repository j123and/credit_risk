{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2: Credit Risk and Statistical Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pVNblDBpVoAq"
   },
   "source": [
    "**Names of all group members:**\n",
    "- Firstname Lastname (email@example.com)\n",
    "- Firstname Lastname (email@example.com)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "All code below is only suggestive and you may as well use different approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K1qPukjnSRgc"
   },
   "outputs": [],
   "source": [
    "# Exercise 1.\n",
    "import numpy as np\n",
    "import sklearn\n",
    "np.random.seed(32)  # for reproducibility\n",
    "\n",
    "# simulate explanatory variables x\n",
    "\n",
    "\n",
    "# a) calculate empirical means and standard deviations over training data\n",
    "\n",
    "\n",
    "# b) Suggest other variables that would realistically be relevant in credit scoring.\n",
    "# (you do not have to implement those of course, just explain your answer in writing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params \n",
    "m = 20000\n",
    "n = 10000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = np.array([np.random.uniform(18,80) for _ in range(m+n)])\n",
    "x_2 = np.array([np.random.uniform(1,15) for _ in range(m+n)])\n",
    "x_3 = np.array([int(np.random.uniform(0,1)>0.1) for _ in range(m+n)])\n",
    "\n",
    "x = np.array([x_1,x_2,x_3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x[0][0],x[1][0],x[2][0])\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_x1 = np.mean(x_1[:m])\n",
    "mu_x2 = np.mean(x_2[:m])\n",
    "mu_x3 = np.mean(x_3[:m])\n",
    "\n",
    "sigma_x1 = np.std(x_1[:m])\n",
    "sigma_x2 = np.std(x_2[:m])\n",
    "sigma_x3 = np.std(x_3[:m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mu_x1,mu_x2,mu_x3)\n",
    "print(sigma_x1,sigma_x2,sigma_x3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other interesting parameters might be: education level, loans and assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RhR3TshATJOo"
   },
   "outputs": [],
   "source": [
    "# Exercise 2.\n",
    "# Building the datasets:\n",
    "\n",
    "sigmoid = lambda x: 1. / (1. + np.exp(-x))\n",
    "\n",
    "# build the first dataset\n",
    "\n",
    "# build the second dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xi = [np.random.uniform(0,1) for _ in range(n+m)]\n",
    "p1 = lambda x1,x2,x3: sigmoid(13.3 -0.33*x1+3.5*x2-3*x3)\n",
    "p2 = lambda x1,x2,x3: sigmoid(5-10*(int(x1<25)+int(x1>75))+1.1*x2-x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = [int(xi[i]<=p1(x_1[i],x_2[i],x_3[i])) for i in range(m+n)]\n",
    "y2 = [int(xi[i]<=p2(x_1[i],x_2[i],x_3[i])) for i in range(m+n)]\n",
    "\n",
    "x = np.column_stack(x)\n",
    "d1 = [x,y1]\n",
    "d2 = [x,y2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(d1))\n",
    "print(len(d1[0]))\n",
    "print(type(d1[0][0]))\n",
    "print(d1[0][0].shape)\n",
    "print(len(d1[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phat1, phat2 = sklearn.linear_model.LogisticRegression(), sklearn.linear_model.LogisticRegression()\n",
    "\n",
    "\n",
    "phat1.fit(d1[0][:m],d1[1][:m])\n",
    "phat2.fit(d2[0][:m],d2[1][:m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1_train_probs = phat1.predict_proba(x[:m])\n",
    "loss1 = sklearn.metrics.log_loss(y1[:m],y1_train_probs)\n",
    "\n",
    "y2_train_probs = phat2.predict_proba(x[:m])\n",
    "loss2 = sklearn.metrics.log_loss(y2[:m],y2_train_probs)\n",
    "\n",
    "\n",
    "print(loss1, loss2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1_test_probs = phat1.predict_proba(x[m:])\n",
    "loss1 = sklearn.metrics.log_loss(y1[m:],y1_test_probs)\n",
    "\n",
    "y2_test_probs = phat2.predict_proba(x[m:])\n",
    "loss2 = sklearn.metrics.log_loss(y2[m:],y2_test_probs)\n",
    "\n",
    "\n",
    "print(loss1, loss2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rz30ywUHUOSG"
   },
   "outputs": [],
   "source": [
    "# Exercise 2. a)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "# \"model = LogisticRegression().fit(X_data, Y_data)\" fits a model\n",
    "# \"pred_X = model.predict_proba(X)\" evaluates the model\n",
    "# (note that it outputs both P(Y=0|X) and P(Y=1|X))\n",
    "# \"log_loss(Y, pred_X)\" evaluates the negative conditional log likelihood (also called cross-entropy loss)\n",
    "\n",
    "# Fit the models on both datasets\n",
    "\n",
    "\n",
    "# Calculate cross-entropy loss on both datasets for train and test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZJawLeRKWIQJ"
   },
   "outputs": [],
   "source": [
    "# Exercise 2.b)\n",
    "# Calculate normalized data\n",
    "x = [x_1/sigma_x1, x_2/sigma_x2, x_3/sigma_x3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QaQx76yCWOa3"
   },
   "outputs": [],
   "source": [
    "# Exercise 2.b)\n",
    "from sklearn.svm import SVC\n",
    "# \"model = SVC(kernel='rbf', gamma=GAMMA, C=C, probability=True)\" creates\n",
    "# a model with kernel exp(-GAMMA \\|x-x'\\|_2^2) and regul. parameter C (note the relation between C and the parameter lambda).\n",
    "# \"probability=True\" enables the option \"model.predict_proba(X)\" to predict probabilities from the regression function \\hat{f}^{svm}.\n",
    "# \"model.fit(X, Y)\" optimizes the model parameters (using hinge loss)\n",
    "\n",
    "# Fit the models for both datasets (this can take up to 60 seconds with SVC)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mS2tjfLdYO4Z"
   },
   "outputs": [],
   "source": [
    "# Exercise 2.b)\n",
    "# \"model.predict_proba(X)\" predicts probabilities from features (note that it outputs both P(Y=0|X) and P(Y=1|X))\n",
    "\n",
    "# Calculate cross-entropy loss on both datasets for train and test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7xTDD8SNZhZz"
   },
   "outputs": [],
   "source": [
    "# Exercise 2.c)\n",
    "import matplotlib.pyplot as plt\n",
    "# To calculate the curves, it is fine to take 100 threshold values c, i.e.,\n",
    "ths = np.linspace(0, 1, 100)\n",
    "\n",
    "# To approximately calculate the AUC, it is fine to simply use Riemann sums.\n",
    "# This means, if you have 100 (a_i, b_i) pairs for the curves, a_1 <= a_2 <= ...\n",
    "# then you may simply use the sum\n",
    "# sum_{i=1}^99 (b_i + b_{i+1})/2 * (a_{i+1}-a_i)\n",
    "# as the approximation of the integral (or AUC)\n",
    "\n",
    "\n",
    "# first data set & logistic regression:\n",
    "# (the code should be reusable for all cases, only exchanging datasets and predicted probabilities depending on the model)\n",
    "\n",
    "# Compute and plot the ROC and AUC cruves\n",
    "\n",
    "\n",
    "# second data set & logistic regression:\n",
    "\n",
    "\n",
    "# first data set and SVM:\n",
    "\n",
    "\n",
    "# second data set and SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5YvGSeDoc9ZS"
   },
   "outputs": [],
   "source": [
    "# Exercise 3.\n",
    "\n",
    "# Set model parameters and define matrix D\n",
    "\n",
    "\n",
    "# Scenario 1:\n",
    "# Define Portfolio and possible outcomes for this portfolio using matrix D\n",
    "\n",
    "\n",
    "# Plot the histogram of profits and losses\n",
    "\n",
    "\n",
    "# Calculate expected profit and losses, compute 95%-VaR and 95%-ES\n",
    "\n",
    "\n",
    "# Scenario 2:\n",
    "# Define Portfolio and possible outcomes using the matrix D and the predicted default probabilities from the logistic regression model\n",
    "\n",
    "\n",
    "# Plot the histogram of profits and losses\n",
    "\n",
    "\n",
    "# Calculate expected profit and losses, compute 95%-VaR and 95%-ES\n",
    "\n",
    "\n",
    "# Scenario 3:\n",
    "# Define Portfolio and possible outcomes using the matrix D and the predicted default probabilities from the SVM model\n",
    "\n",
    "\n",
    "# Plot the histogram of profits and losses\n",
    "\n",
    "\n",
    "# Calculate expected profit and losses, compute 95%-VaR and 95%-ES"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
